{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using with `pyspark` for data preprocessing\n",
    "\n",
    "We want to use pyspark to preprocess a potentially huge dataset used for web-marketing.\n",
    "\n",
    "## Data description\n",
    "\n",
    "The data is a `parquet` file which contains a dataframe with 8 columns:\n",
    "\n",
    "- `xid`: unique user id\n",
    "- `action`: type of action. 'C' is a click, 'O' or 'VSL' is a web-display\n",
    "- `date`: date of the action\n",
    "- `website_id`: unique id of the website\n",
    "- `url`: url of the webpage\n",
    "- `category_id`: id of the display\n",
    "- `zipcode`: postal zipcode of the user\n",
    "- `device`: type of device used by the user\n",
    "\n",
    "## Q1. Some statistics / computations\n",
    "\n",
    "Using `pyspark.sql` we want to do the following things:\n",
    "\n",
    "1. Compute the total number of unique users\n",
    "2. Construct a column containing the total number of actions per user\n",
    "3. Construct a column containing the number of days since the last action of the user\n",
    "4. Construct a column containing the number of actions of each user for each modality of device \n",
    "\n",
    "## Q2. Binary classification\n",
    "\n",
    "Then, we want to construct a classifier to predict the click on the category 1204. \n",
    "Here is an agenda for this:\n",
    "\n",
    "1. Construction of a features matrix for which each line corresponds to the information concerning a user.\n",
    "2. In this matrix, we need to keep only the users that have been exposed to the display in category 1204\n",
    "3. Using this training dataset, train a binary classifier, and evaluate your classifier using a precision / recall curve computed on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download/read the data and a first look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T16:25:56.650024Z",
     "start_time": "2020-05-03T16:25:52.400542Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"Web data\")         \n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T16:28:38.406210Z",
     "start_time": "2020-05-03T16:28:08.594803Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path('webdata.parquet')\n",
    "if not path.exists():\n",
    "    url = \"https://stephanegaiffas.github.io/big_data_course/data/webdata.parquet.zip\"\n",
    "    r = requests.get(url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(path='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T16:27:49.141388Z",
     "start_time": "2020-05-03T16:27:48.989135Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_path = './'\n",
    "\n",
    "input_file = os.path.join(input_path, 'webdata.parquet')\n",
    "df = spark.read.parquet(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T16:27:52.121782Z",
     "start_time": "2020-05-03T16:27:50.752210Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic statistics\n",
    "\n",
    "First we need to import some things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:20:34.024704Z",
     "start_time": "2020-05-03T15:20:34.016322Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the total number of unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:20:36.790893Z",
     "start_time": "2020-05-03T15:20:34.856924Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.select('xid').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a column containing the total number of actions per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:20:52.163321Z",
     "start_time": "2020-05-03T15:20:50.965856Z"
    }
   },
   "outputs": [],
   "source": [
    "xid_partition = Window.partitionBy('xid')\n",
    "n_events = func.count(col('action')).over(xid_partition)\n",
    "df = df.withColumn('n_events', n_events)\n",
    "df.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a column containing the number of days since the last action of the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:20:55.895918Z",
     "start_time": "2020-05-03T15:20:54.925126Z"
    }
   },
   "outputs": [],
   "source": [
    "xid_partition = Window.partitionBy('xid')\n",
    "max_date = func.max(col('date')).over(xid_partition)\n",
    "n_days_since_last_event = func.datediff(func.current_date(), max_date)\n",
    "df = df.withColumn('n_days_since_last_event',\n",
    "                   n_days_since_last_event)\n",
    "df.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a column containing the number of actions of each user for each modality of device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:21:03.846417Z",
     "start_time": "2020-05-03T15:21:03.027855Z"
    }
   },
   "outputs": [],
   "source": [
    "xid_device_partition = Window.partitionBy('xid', 'device')\n",
    "n_events_per_device = func.count(col('action')).over(xid_device_partition)\n",
    "df = df.withColumn('n_events_per_device', n_events_per_device)\n",
    "df.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of device per user: some mental gymnastics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:21:06.411472Z",
     "start_time": "2020-05-03T15:21:05.373879Z"
    }
   },
   "outputs": [],
   "source": [
    "xid_partition = Window.partitionBy('xid')\n",
    "rank_device = func.dense_rank().over(xid_partition.orderBy('device'))\n",
    "n_unique_device = func.last(rank_device).over(xid_partition)\n",
    "df = df.withColumn('n_device', n_unique_device)\n",
    "df.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:21:13.863382Z",
     "start_time": "2020-05-03T15:21:12.479688Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df\\\n",
    "    .where(col('n_device') > 1)\\\n",
    "    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n",
    "    .head(n=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's select the correct users and build a training dataset\n",
    "\n",
    "We construct a ETL (Extract Transform Load) process on this data using the `pyspark.sql` API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction\n",
    "\n",
    "Extraction is easy here, it's just about reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:21:36.683625Z",
     "start_time": "2020-05-03T15:21:36.427615Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(input_file)\n",
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation of the data\n",
    "\n",
    "At this step we compute a lot of extra things from the data. The aim is to build features that describe users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:24:09.159215Z",
     "start_time": "2020-05-03T15:24:09.136189Z"
    }
   },
   "outputs": [],
   "source": [
    "def n_events_transformer(df):\n",
    "    xid_partition = Window.partitionBy('xid')\n",
    "    n_events = func.count(col('action')).over(xid_partition)\n",
    "    df = df.withColumn('n_events', n_events)\n",
    "    return df\n",
    "\n",
    "def n_events_per_action_transformer(df):\n",
    "    xid_action_partition = Window.partitionBy('xid', 'action')\n",
    "    n_events_per_action = func.count(col('action')).over(xid_action_partition)\n",
    "    df = df.withColumn('n_events_per_action', n_events_per_action)\n",
    "    return df\n",
    "\n",
    "def hour_transformer(df):\n",
    "    hour = func.hour(col('date'))\n",
    "    df = df.withColumn('hour', hour)\n",
    "    return df\n",
    "\n",
    "def weekday_transformer(df):\n",
    "    weekday = func.date_format(col('date'), 'EEEE')\n",
    "    df = df.withColumn('weekday', weekday)\n",
    "    return df\n",
    "\n",
    "def n_events_per_hour_transformer(df):\n",
    "    xid_hour_partition = Window.partitionBy('xid', 'hour')\n",
    "    n_events_per_hour = func.count(col('action')).over(xid_hour_partition)\n",
    "    df = df.withColumn('n_events_per_hour', n_events_per_hour)\n",
    "    return df\n",
    "\n",
    "def n_events_per_weekday_transformer(df):\n",
    "    xid_weekday_partition = Window.partitionBy('xid', 'weekday')\n",
    "    n_events_per_weekday = func.count(col('action')).over(xid_weekday_partition)\n",
    "    df = df.withColumn('n_events_per_weekday', n_events_per_weekday)\n",
    "    return df\n",
    "\n",
    "def n_days_since_last_event_transformer(df):\n",
    "    xid_partition = Window.partitionBy('xid')\n",
    "    max_date = func.max(col('date')).over(xid_partition)\n",
    "    n_days_since_last_event = func.datediff(func.current_date(), max_date)\n",
    "    df = df.withColumn('n_days_since_last_event',\n",
    "                       n_days_since_last_event + lit(0.1))\n",
    "    return df\n",
    "\n",
    "def n_days_since_last_action_transformer(df):\n",
    "    xid_partition_action = Window.partitionBy('xid', 'action')\n",
    "    max_date = func.max(col('date')).over(xid_partition_action)\n",
    "    n_days_since_last_action = func.datediff(func.current_date(),\n",
    "                                                        max_date)\n",
    "    df = df.withColumn('n_days_since_last_action',\n",
    "                       n_days_since_last_action + lit(0.1))\n",
    "    return df\n",
    "\n",
    "def n_unique_day_transformer(df):\n",
    "    xid_partition = Window.partitionBy('xid')\n",
    "    dayofyear = func.dayofyear(col('date'))\n",
    "    rank_day = func.dense_rank().over(xid_partition.orderBy(dayofyear))\n",
    "    n_unique_day = func.last(rank_day).over(xid_partition)\n",
    "    df = df.withColumn('n_unique_day', n_unique_day)\n",
    "    return df\n",
    "\n",
    "def n_unique_hour_transformer(df):\n",
    "    xid_partition = Window.partitionBy('xid')\n",
    "    rank_hour = func.dense_rank().over(xid_partition.orderBy('hour'))\n",
    "    n_unique_hour = func.last(rank_hour).over(xid_partition)\n",
    "    df = df.withColumn('n_unique_hour', n_unique_hour)\n",
    "    return df\n",
    "\n",
    "def n_events_per_device_transformer(df):\n",
    "    xid_device_partition = Window.partitionBy('xid', 'device')\n",
    "    n_events_per_device = func.count(func.col('device')) \\\n",
    "        .over(xid_device_partition)\n",
    "    df = df.withColumn('n_events_per_device', n_events_per_device)\n",
    "    return df\n",
    "\n",
    "def n_unique_device_transformer(df):\n",
    "    xid_partition = Window.partitionBy('xid')\n",
    "    rank_device = func.dense_rank().over(xid_partition.orderBy('device'))\n",
    "    n_unique_device = func.last(rank_device).over(xid_partition)\n",
    "    df = df.withColumn('n_device', n_unique_device)\n",
    "    return df\n",
    "\n",
    "def n_actions_per_category_id_transformer(df):\n",
    "    xid_category_id_partition = Window.partitionBy('xid', 'category_id',\n",
    "                                                   'action')\n",
    "    n_actions_per_category_id = func.count(func.col('action')) \\\n",
    "        .over(xid_category_id_partition)\n",
    "    df = df.withColumn('n_actions_per_category_id', n_actions_per_category_id)\n",
    "    return df\n",
    "\n",
    "def n_unique_category_id_transformer(df):\n",
    "    xid_partition = Window.partitionBy('xid')\n",
    "    rank_category_id = func.dense_rank().over(xid_partition\\\n",
    "                                              .orderBy('category_id'))\n",
    "    n_unique_category_id = func.last(rank_category_id).over(xid_partition)\n",
    "    df = df.withColumn('n_unique_category_id', n_unique_category_id)\n",
    "    return df\n",
    "\n",
    "def n_events_per_category_id_transformer(df):\n",
    "    xid_category_id_partition = Window.partitionBy('xid', 'category_id')\n",
    "    n_events_per_category_id = func.count(func.col('action')) \\\n",
    "        .over(xid_category_id_partition)\n",
    "    df = df.withColumn('n_events_per_category_id', n_events_per_category_id)\n",
    "    return df\n",
    "\n",
    "def n_events_per_website_id_transformer(df):\n",
    "    xid_website_id_partition = Window.partitionBy('xid', 'website_id')\n",
    "    n_events_per_website_id = func.count(col('action'))\\\n",
    "        .over(xid_website_id_partition)\n",
    "    df = df.withColumn('n_events_per_website_id', n_events_per_website_id)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:24:33.042444Z",
     "start_time": "2020-05-03T15:24:15.032735Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transformers = [\n",
    "    hour_transformer,\n",
    "    weekday_transformer,\n",
    "    n_events_per_hour_transformer,\n",
    "    n_events_per_weekday_transformer,\n",
    "    n_days_since_last_event_transformer,\n",
    "    n_days_since_last_action_transformer,\n",
    "    n_unique_day_transformer,\n",
    "    n_unique_hour_transformer,\n",
    "    n_events_per_device_transformer,\n",
    "    n_unique_device_transformer,\n",
    "    n_actions_per_category_id_transformer,\n",
    "    n_events_per_category_id_transformer,\n",
    "    n_events_per_website_id_transformer,\n",
    "]\n",
    "\n",
    "for transformer in transformers:\n",
    "    df = transformer(df)\n",
    "\n",
    "df.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:24:40.406910Z",
     "start_time": "2020-05-03T15:24:40.393184Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load step\n",
    "\n",
    "Here, we use all the previous computations (saved in the columns of the dataframe) \n",
    "to compute aggregated informations about each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:25:22.243433Z",
     "start_time": "2020-05-03T15:25:22.217248Z"
    }
   },
   "outputs": [],
   "source": [
    "def n_events_per_hour_loader(df):\n",
    "    csr = df\\\n",
    "        .select('xid', 'hour', 'n_events_per_hour')\\\n",
    "        .withColumnRenamed('n_events_per_hour', 'value')\\\n",
    "        .distinct()\n",
    "    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n",
    "    csr = csr\\\n",
    "        .withColumn('feature_name', feature_name)\\\n",
    "        .drop('hour')\n",
    "    return csr\n",
    "\n",
    "def n_events_per_website_id_loader(df):\n",
    "    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n",
    "        .withColumnRenamed('n_events_per_hour', 'value')\\\n",
    "        .distinct()\n",
    "    feature_name = func.concat(lit('n_events_per_website_id#'),\n",
    "                               col('website_id'))\n",
    "    csr = csr\\\n",
    "        .withColumn('feature_name', feature_name)\\\n",
    "        .drop('website_id')\n",
    "    return csr\n",
    "\n",
    "def n_events_per_hour_loader(df):\n",
    "    csr = df\\\n",
    "        .select('xid', 'hour', 'n_events_per_hour')\\\n",
    "        .withColumnRenamed('n_events_per_hour', 'value')\\\n",
    "        .distinct()\n",
    "    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n",
    "    csr = csr\\\n",
    "        .withColumn('feature_name', feature_name)\\\n",
    "        .drop('hour')\n",
    "    return csr\n",
    "\n",
    "def n_events_per_weekday_loader(df):\n",
    "    csr = df\\\n",
    "        .select('xid', 'weekday', 'n_events_per_weekday')\\\n",
    "        .withColumnRenamed('n_events_per_weekday', 'value')\\\n",
    "        .distinct()\n",
    "    feature_name = func.concat(lit('n_events_per_weekday#'), col('weekday'))\n",
    "    csr = csr\\\n",
    "        .withColumn('feature_name', feature_name)\\\n",
    "        .drop('weekday')\n",
    "    return csr\n",
    "\n",
    "def n_days_since_last_event_loader(df):\n",
    "    csr = df.select('xid',  'n_days_since_last_event')\\\n",
    "        .withColumnRenamed('n_days_since_last_event#', 'value')\\\n",
    "        .distinct()\n",
    "    feature_name = lit('n_days_since_last_event')\n",
    "    csr = csr\\\n",
    "        .withColumn('feature_name', feature_name)\n",
    "    return csr\n",
    "\n",
    "def n_days_since_last_action_loader(df):\n",
    "    csr = df.select('xid', 'action', 'n_days_since_last_action')\\\n",
    "        .withColumnRenamed('n_days_since_last_action', 'value')\\\n",
    "        .distinct()\n",
    "    feature_name = func.concat(lit('n_days_since_last_action#'), col('action'))\n",
    "    csr = csr\\\n",
    "        .withColumn('feature_name', feature_name)\\\n",
    "        .drop('action')\n",
    "    return csr\n",
    "\n",
    "def n_unique_day_loader(df):\n",
    "    csr = df.select('xid', 'n_unique_day')\\\n",
    "        .withColumnRenamed('n_unique_day', 'value')\\\n",
    "        .distinct()\n",
    "    feature_name = lit('n_unique_day')\n",
    "    csr = csr\\\n",
    "        .withColumn('feature_name', feature_name)\n",
    "    return csr\n",
    "\n",
    "def n_unique_hour_loader(df):\n",
    "    csr = df.select('xid', 'n_unique_hour')\\\n",
    "        .withColumnRenamed('n_unique_hour', 'value')\\\n",
    "        .distinct()\n",
    "    feature_name = lit('n_unique_hour')\n",
    "    csr = csr\\\n",
    "        .withColumn('feature_name', feature_name)\n",
    "    return csr\n",
    "\n",
    "def n_events_per_device_loader(df):\n",
    "    csr = df\\\n",
    "        .select('xid', 'device', 'n_events_per_device')\\\n",
    "        .withColumnRenamed('n_events_per_device', 'value')\\\n",
    "        .distinct()\n",
    "    feature_name = func.concat(lit('n_events_per_device#'), col('device'))\n",
    "    csr = csr\\\n",
    "        .withColumn('feature_name', feature_name)\\\n",
    "        .drop('device')\n",
    "    return csr\n",
    "\n",
    "def n_unique_device_loader(df):\n",
    "    csr = df.select('xid', 'n_device')\\\n",
    "        .withColumnRenamed('n_device', 'value')\\\n",
    "        .distinct()\n",
    "    feature_name = lit('n_device')\n",
    "    csr = csr\\\n",
    "        .withColumn('feature_name', feature_name)\n",
    "    return csr\n",
    "\n",
    "def n_events_per_category_id_loader(df):\n",
    "    csr = df.select('xid', 'category_id', 'n_events_per_category_id')\\\n",
    "        .withColumnRenamed('n_events_per_category_id', 'value')\\\n",
    "        .distinct()\n",
    "    feature_name = func.concat(lit('n_events_per_category_id#'),\n",
    "                               col('category_id'))\n",
    "    csr = csr\\\n",
    "        .withColumn('feature_name', feature_name)\\\n",
    "        .drop('category_id')\n",
    "    return csr\n",
    "\n",
    "def n_actions_per_category_id_loader(df):\n",
    "    csr = df.select('xid', 'category_id', 'action', 'n_actions_per_category_id')\\\n",
    "        .withColumnRenamed('n_actions_per_category_id', 'value')\\\n",
    "        .distinct()\n",
    "    feature_name = func.concat(lit('n_actions_per_category_id#'),\n",
    "                               col('action'), lit('#'), \n",
    "                               col('category_id'))\n",
    "    csr = csr\\\n",
    "        .withColumn('feature_name', feature_name)\\\n",
    "        .drop('category_id')\\\n",
    "        .drop('action')\n",
    "    return csr\n",
    "\n",
    "def n_events_per_website_id_loader(df):\n",
    "    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n",
    "        .withColumnRenamed('n_events_per_website_id', 'value')\\\n",
    "        .distinct()\n",
    "    feature_name = func.concat(lit('n_events_per_website_id#'),\n",
    "                               col('website_id'))\n",
    "    csr = csr\\\n",
    "        .withColumn('feature_name', feature_name)\\\n",
    "        .drop('website_id')\n",
    "    return csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:25:48.149670Z",
     "start_time": "2020-05-03T15:25:37.966657Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "loaders = [\n",
    "    n_events_per_hour_loader,\n",
    "    n_events_per_website_id_loader,\n",
    "    n_events_per_hour_loader,\n",
    "    n_events_per_weekday_loader,\n",
    "    n_days_since_last_event_loader,\n",
    "    n_days_since_last_action_loader,\n",
    "    n_unique_day_loader,\n",
    "    n_unique_hour_loader,\n",
    "    n_events_per_device_loader,\n",
    "    n_unique_device_loader,\n",
    "    n_events_per_category_id_loader,\n",
    "    n_actions_per_category_id_loader,\n",
    "    n_events_per_website_id_loader,\n",
    "]\n",
    "\n",
    "def union(df, other):\n",
    "    return df.union(other)\n",
    "\n",
    "csr = reduce(\n",
    "    lambda df1, df2: df1.union(df2),\n",
    "    [loader(df) for loader in loaders]\n",
    ")\n",
    "\n",
    "csr.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:25:54.862814Z",
     "start_time": "2020-05-03T15:25:54.857914Z"
    }
   },
   "outputs": [],
   "source": [
    "csr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:26:13.629146Z",
     "start_time": "2020-05-03T15:25:55.683800Z"
    }
   },
   "outputs": [],
   "source": [
    "csr.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:30:20.643141Z",
     "start_time": "2020-05-03T15:29:45.221790Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Replace features names and xid by a unique number\n",
    "feature_name_partition = Window().orderBy('feature_name')\n",
    "xid_partition = Window().orderBy('xid')\n",
    "\n",
    "col_idx = func.dense_rank().over(feature_name_partition)\n",
    "row_idx = func.dense_rank().over(xid_partition)\n",
    "\n",
    "csr = csr.withColumn('col', col_idx)\\\n",
    "    .withColumn('row', row_idx)\n",
    "\n",
    "csr = csr.na.drop('any')\n",
    "\n",
    "csr.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:32:02.552364Z",
     "start_time": "2020-05-03T15:31:14.990298Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's save the result of our hard work into a new parquet file\n",
    "output_path = './'\n",
    "output_file = os.path.join(output_path, 'csr.parquet')\n",
    "csr.write.parquet(output_file, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:32:56.421452Z",
     "start_time": "2020-05-03T15:32:55.819071Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csr_path = './'\n",
    "csr_file = os.path.join(csr_path, 'csr.parquet')\n",
    "\n",
    "df = spark.read.parquet(csr_file)\n",
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:33:17.229477Z",
     "start_time": "2020-05-03T15:33:16.995048Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:33:20.881392Z",
     "start_time": "2020-05-03T15:33:19.624525Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What are the features related to campaign_id 1204 ?\n",
    "features_names = \\\n",
    "    df.select('feature_name')\\\n",
    "    .distinct()\\\n",
    "    .toPandas()['feature_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:33:21.818568Z",
     "start_time": "2020-05-03T15:33:21.812810Z"
    }
   },
   "outputs": [],
   "source": [
    "features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:33:27.083141Z",
     "start_time": "2020-05-03T15:33:27.078374Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[feature_name for feature_name in features_names if '1204' in feature_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:33:28.560631Z",
     "start_time": "2020-05-03T15:33:27.903921Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look for the xid that have at least one exposure to campaign 1204\n",
    "keep = func.when(\n",
    "    (col('feature_name') == 'n_actions_per_category_id#C#1204.0') |\n",
    "    (col('feature_name') == 'n_actions_per_category_id#O#1204.0'),\n",
    "    1).otherwise(0)\n",
    "df = df.withColumn('keep', keep)\n",
    "\n",
    "df.where(col('keep') > 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:33:31.274277Z",
     "start_time": "2020-05-03T15:33:31.244066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sum of the keeps :)\n",
    "xid_partition = Window.partitionBy('xid')\n",
    "sum_keep = func.sum(col('keep')).over(xid_partition)\n",
    "df = df.withColumn('sum_keep', sum_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:33:31.467139Z",
     "start_time": "2020-05-03T15:33:31.404561Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's keep the xid exposed to 1204\n",
    "df = df.where(col('sum_keep') > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:33:34.619928Z",
     "start_time": "2020-05-03T15:33:31.572475Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:33:37.918500Z",
     "start_time": "2020-05-03T15:33:34.622711Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select('xid').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:33:52.607777Z",
     "start_time": "2020-05-03T15:33:40.110545Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "row_partition = Window().orderBy('row')\n",
    "col_partition = Window().orderBy('col')\n",
    "row_new = func.dense_rank().over(row_partition)\n",
    "col_new = func.dense_rank().over(col_partition)\n",
    "df = df.withColumn('row_new', row_new)\n",
    "df = df.withColumn('col_new', col_new)\n",
    "csr_data = df.select('row_new', 'col_new', 'value').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:33:52.617724Z",
     "start_time": "2020-05-03T15:33:52.609488Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csr_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:33:58.443120Z",
     "start_time": "2020-05-03T15:33:52.619858Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_names = df.select('feature_name', 'col_new').distinct()\n",
    "features_names.where(col('feature_name') == 'n_actions_per_category_id#C#1204.0').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:34:04.104342Z",
     "start_time": "2020-05-03T15:33:58.445504Z"
    }
   },
   "outputs": [],
   "source": [
    "features_names.where(col('feature_name') == 'n_actions_per_category_id#O#1204.0').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:34:11.510538Z",
     "start_time": "2020-05-03T15:34:11.454802Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "rows = csr_data['row_new'].values - 1\n",
    "cols = csr_data['col_new'].values - 1\n",
    "vals = csr_data['value'].values\n",
    "\n",
    "X_csr = csr_matrix((vals, (rows, cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:34:11.977267Z",
     "start_time": "2020-05-03T15:34:11.972602Z"
    }
   },
   "outputs": [],
   "source": [
    "X_csr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:34:28.207343Z",
     "start_time": "2020-05-03T15:34:28.202443Z"
    }
   },
   "outputs": [],
   "source": [
    "X_csr.shape, X_csr.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:34:30.978599Z",
     "start_time": "2020-05-03T15:34:30.972909Z"
    }
   },
   "outputs": [],
   "source": [
    "X_csr.nnz / (152347 * 92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:34:32.871960Z",
     "start_time": "2020-05-03T15:34:32.860482Z"
    }
   },
   "outputs": [],
   "source": [
    "# The label vector. Let's make it dense, flat and binary\n",
    "y = np.array(X_csr[:, 1].todense()).ravel()\n",
    "y = np.array(y > 0, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:34:33.348181Z",
     "start_time": "2020-05-03T15:34:33.343110Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_csr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:34:37.382059Z",
     "start_time": "2020-05-03T15:34:37.371588Z"
    }
   },
   "outputs": [],
   "source": [
    "# We remove the second and fourth column. \n",
    "# It actually contain the label we'll want to predict.\n",
    "kept_cols = list(range(92))\n",
    "kept_cols.pop(1)\n",
    "kept_cols.pop(2)\n",
    "X = X_csr[:, kept_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:34:38.375629Z",
     "start_time": "2020-05-03T15:34:38.369734Z"
    }
   },
   "outputs": [],
   "source": [
    "X_csr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow ! That was a lot of work. Now we have a features matrix $X$ and a vector of labels $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:34:40.526092Z",
     "start_time": "2020-05-03T15:34:40.521420Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:34:40.750471Z",
     "start_time": "2020-05-03T15:34:40.744670Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.indptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:34:40.974359Z",
     "start_time": "2020-05-03T15:34:40.969638Z"
    }
   },
   "outputs": [],
   "source": [
    "X.shape, X.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:34:41.220722Z",
     "start_time": "2020-05-03T15:34:41.213466Z"
    }
   },
   "outputs": [],
   "source": [
    "y.shape, y.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some learning for this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:51:21.964565Z",
     "start_time": "2020-05-03T15:51:20.939544Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Normalize the features\n",
    "X = MaxAbsScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3)\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=1e3,\n",
    "    solver='lbfgs',\n",
    "    class_weight='balanced'\n",
    ")\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:51:22.820046Z",
     "start_time": "2020-05-03T15:51:22.809009Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_names = features_names.toPandas()['feature_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:51:25.078266Z",
     "start_time": "2020-05-03T15:51:23.622795Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.stem(clf.coef_[0], use_line_collection=True)\n",
    "plt.title('Logistic regression coefficients', fontsize=18)\n",
    "# We change the fontsize of minor ticks label\n",
    "_ = plt.xticks(np.arange(clf.coef_[0].shape[0]), features_names, \n",
    "           rotation='vertical', fontsize=8)\n",
    "_ = plt.yticks(fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T15:51:25.280157Z",
     "start_time": "2020-05-03T15:51:25.081464Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "    \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label='LR (F1=%.2f)' % f1_score(y_test, clf.predict(X_test)), lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall', fontsize=16)\n",
    "plt.ylabel('Precision', fontsize=16)\n",
    "plt.title('Precision/recall curve', fontsize=18)\n",
    "plt.legend(loc=\"upper right\", fontsize=14)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
