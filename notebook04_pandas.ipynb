{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing with `pandas`\n",
    "\n",
    "We'll work on a dataset for credit scoring that was proposed some years ago as a data challenge on some data challenge website.\n",
    "It is a realistic dataset that contains a lot of missing values, several types of features (dates, categories, continuous features), so that a serious data cleaning and formating is required.\n",
    "\n",
    "# Importing the data\n",
    "\n",
    "We start by downloading the data. You need to do this only **once** since the next cell downloads the `gro` dataset. This dataset contains the following columns:\n",
    "\n",
    "| Column name          | Description |\n",
    "|:---------------------|:------------|\n",
    "| BirthDate            | Date de naissance |\n",
    "| Customer_Open_Date   | Date d'arrivée du client dans la filiale bancaire |\n",
    "| Customer_Type        | Type de client (existant / nouveau) | \n",
    "| Educational_Level    | Niveau de diplôme |\n",
    "| Id_Customer          | Identifiant du client |\n",
    "| Marital_Status       | Situation familiale |\n",
    "| Nb_Of_Products       | Nombre de produits détenus par le client à l'octroi |\n",
    "| Net_Annual_Income    | Revenu annuel |\n",
    "| Number_Of_Dependant  | Nombre de personnes à charge |\n",
    "| P_Client             | Distinction des clients selon une caractéristique non définie ici |\n",
    "| Prod_Category        | Catégorie du produit |\n",
    "| Prod_Closed_Date     | Date de fermeture du produit |\n",
    "| Prod_Decision_Date   | Date de décision de l'octroi (du financement) |\n",
    "| Prod_Sub_Category    | Sous-catégorie du produit |\n",
    "| Source               | Source de financement (Branch or Sales) |\n",
    "| Type_Of_Residence    | Situation résidentielle |\n",
    "| Y                    | Crédit accordé ou non | |\n",
    "| Years_At_Business    | Nombre d'années dans son emploi actuel |\n",
    "| Years_At_Residence   | Nombre d'années dans son lieu de résidence actuel |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:08.578479Z",
     "start_time": "2019-12-10T17:20:08.500950Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# The path containing your notebook\n",
    "path_data = './'\n",
    "# The name of the file\n",
    "filename = 'gro.csv.gz'\n",
    "\n",
    "if os.path.exists(os.path.join(path_data, filename)):\n",
    "    print('The file %s already exists.' % os.path.join(path_data, filename))\n",
    "else:\n",
    "    url = 'http://stephanegaiffas.github.io/files/m2mo/gro.csv.gz'\n",
    "    r = requests.get(url)\n",
    "    with open(os.path.join(path_data, filename), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    print('Downloaded file %s.' % os.path.join(path_data, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick and easy (but actually bad) import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the data into a pandas dataframe, as simply as possible\n",
    "The only thing we care about for now is the fact that the column separator \n",
    "is `';'` and not `','` as it should be in a `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:08.974902Z",
     "start_time": "2019-12-10T17:20:08.580793Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(os.path.join(path_data, filename), sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:08.987458Z",
     "start_time": "2019-12-10T17:20:08.977181Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.022711Z",
     "start_time": "2019-12-10T17:20:08.990213Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's look at the first lines of the dataframe\n",
    "data.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.032157Z",
     "start_time": "2019-12-10T17:20:09.025329Z"
    }
   },
   "outputs": [],
   "source": [
    "# The list of column names\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**. There are weird columns in the end, they look empty. \n",
    "They don't appear in the description of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.041889Z",
     "start_time": "2019-12-10T17:20:09.034316Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's look at the types used for each column\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.051218Z",
     "start_time": "2019-12-10T17:20:09.044489Z"
    }
   },
   "outputs": [],
   "source": [
    "data.BirthDate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.059568Z",
     "start_time": "2019-12-10T17:20:09.054708Z"
    }
   },
   "outputs": [],
   "source": [
    "# Birth date is imported as a string (all columns containing dates do)\n",
    "type(data['BirthDate'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.068478Z",
     "start_time": "2019-12-10T17:20:09.062546Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data['Prod_Sub_Category'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.076151Z",
     "start_time": "2019-12-10T17:20:09.070638Z"
    }
   },
   "outputs": [],
   "source": [
    "# Product sub category is imported as a string\n",
    "type(data['Prod_Sub_Category'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.084366Z",
     "start_time": "2019-12-10T17:20:09.078211Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data['Net_Annual_Income'].head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.091489Z",
     "start_time": "2019-12-10T17:20:09.086334Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Net actual income is a string as well ! While it is clearly a number\n",
    "type(data['Net_Annual_Income'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.184142Z",
     "start_time": "2019-12-10T17:20:09.093660Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Quick summary statistics of the dataset features'\n",
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's assess what we did\n",
    "\n",
    "It appears that we have to work a little bit more. \n",
    "Here is the list of problems we have.\n",
    "- The last three columns are weird and empty\n",
    "- Dates are actually `str` (python's string type)\n",
    "- There is a lot of missing values\n",
    "- Categorial features are `str`\n",
    "- The annual net income is imported as a string\n",
    "\n",
    "By looking at the column names, basic statistics and the dataset \n",
    "description, we infer the type of features that we have.\n",
    "There are dates features, continuous features, categorical features, and\n",
    "some features could be either treated as categorical or continuous.\n",
    "\n",
    "There is **a lot** of missing values, that need to be handled somehow.\n",
    "\n",
    "The annual net income is imported as a string, we need to understand why.\n",
    "\n",
    "We really need to treat dates as dates and not string (because we'd want to compute the age of a client based on its birth year for instance).\n",
    "\n",
    "Here is the structure of the features.\n",
    "\n",
    "**Continuous features**\n",
    "- Years_At_Residence\n",
    "- Net_Annual_Income\n",
    "- Years_At_Business\n",
    "\n",
    "**To decide features**\n",
    "- Number_Of_Dependant\n",
    "- Nb_Of_Products\n",
    "\n",
    "**Categorical features are**\n",
    "- Customer_Type\n",
    "- P_Client\n",
    "- Educational_Level\n",
    "- Marital_Status\n",
    "- Prod_Sub_Category\n",
    "- Source\n",
    "- Type_Of_Residence\n",
    "- Prod_Category\n",
    "\n",
    "**Date features are**\n",
    "- BirthDate\n",
    "- Customer_Open_Date\n",
    "- Prod_Decision_Date\n",
    "- Prod_Closed_Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A closer look at the import problems\n",
    "\n",
    "Let's find solutions to all these import problems.\n",
    "\n",
    "## The last three columns are weird and empty \n",
    "\n",
    "It seems to come from the fact that the data always ends with several `';'` characters. Let's simply remove them using the `usecols`\n",
    "option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates are actually `str` (python's string type)\n",
    "\n",
    "We need to specify which columns must be encoded as dates using the `parse_dates` option. Fortunately enough, the `pandas` is clever enough to interpret the date format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.193148Z",
     "start_time": "2019-12-10T17:20:09.186689Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(data['BirthDate'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There is a lot of missing values \n",
    "\n",
    "We'll see below that actually a single column mostly contain missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.209062Z",
     "start_time": "2019-12-10T17:20:09.195270Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.221117Z",
     "start_time": "2019-12-10T17:20:09.211052Z"
    }
   },
   "outputs": [],
   "source": [
    "# The \"product closed date\" seems to contain a lot of missing values !\n",
    "data[['Prod_Closed_Date']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.228906Z",
     "start_time": "2019-12-10T17:20:09.223390Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We remove the useless columns from the dataframe\n",
    "data = data.drop(['Prod_Closed_Date', 'Unnamed: 19', \n",
    "                            'Unnamed: 20', 'Unnamed: 21'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.250679Z",
     "start_time": "2019-12-10T17:20:09.231328Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.278435Z",
     "start_time": "2019-12-10T17:20:09.252663Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The remaining lines with missing values\n",
    "data[data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorial features are `str`\n",
    "\n",
    "We need to say the dtype we want to use for some columns using the `dtype` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.286906Z",
     "start_time": "2019-12-10T17:20:09.280545Z"
    }
   },
   "outputs": [],
   "source": [
    "# Product sub category is imported as a string\n",
    "type(data['Prod_Sub_Category'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.295407Z",
     "start_time": "2019-12-10T17:20:09.289263Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['Prod_Sub_Category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The annual net income is imported as a string\n",
    "\n",
    "This problem comes from the fact that the decimal separator is in European notation: it's a `','` and not a `'.'`, so we need to specify it using the `decimal` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.302963Z",
     "start_time": "2019-12-10T17:20:09.297394Z"
    }
   },
   "outputs": [],
   "source": [
    "type(data['Net_Annual_Income'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.313822Z",
     "start_time": "2019-12-10T17:20:09.307614Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data['Net_Annual_Income'].head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A better import of the data\n",
    "\n",
    "## Let's import again the data\n",
    "\n",
    "- We build a dict that specifies the types to use for each column \n",
    "and pass it to `read_csv` using the `dtype` option\n",
    "- We also specify the `decimal`, `usecols` and `parse_dates` options.\n",
    "\n",
    "**Important remark (expert)**. Some columns could be `np.int`. \n",
    "However, `pandas` (actually its `numpy`) does not support columns \n",
    "with integer dtype and missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.370742Z",
     "start_time": "2019-12-10T17:20:09.316725Z"
    }
   },
   "outputs": [],
   "source": [
    "gro_dtypes = {\n",
    "    'Years_At_Residence': np.int,\n",
    "    'Net_Annual_Income' : np.float,\n",
    "    'Years_At_Business': np.float,\n",
    "    'Number_Of_Dependant': np.float,\n",
    "    'Nb_Of_Products': np.int,\n",
    "    'Customer_Type': 'category',\n",
    "    'P_Client': 'category',\n",
    "    'Educational_Level': 'category',\n",
    "    'Marital_Status': 'category',\n",
    "    'Prod_Sub_Category': 'category',\n",
    "    'Source': 'category',\n",
    "    'Type_Of_Residence': 'category',\n",
    "    'Prod_Category': 'category',\n",
    "}\n",
    "\n",
    "data = pd.read_csv(os.path.join(path_data, filename), \n",
    "                   sep=';', decimal=',', usecols=range(19),\n",
    "                   parse_dates=['BirthDate', 'Customer_Open_Date', \n",
    "                                'Prod_Decision_Date', 'Prod_Closed_Date'],\n",
    "                   dtype=gro_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.377548Z",
     "start_time": "2019-12-10T17:20:09.372205Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.390722Z",
     "start_time": "2019-12-10T17:20:09.380960Z"
    }
   },
   "outputs": [],
   "source": [
    "# Product subcategory is imported as a string as well\n",
    "data['Prod_Sub_Category'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.399911Z",
     "start_time": "2019-12-10T17:20:09.392671Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data['BirthDate'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.408456Z",
     "start_time": "2019-12-10T17:20:09.402432Z"
    }
   },
   "outputs": [],
   "source": [
    "data['BirthDate'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.416874Z",
     "start_time": "2019-12-10T17:20:09.410550Z"
    }
   },
   "outputs": [],
   "source": [
    "# So let's simply remove this column for now\n",
    "prod_closed_date = data.pop('Prod_Closed_Date')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.429320Z",
     "start_time": "2019-12-10T17:20:09.419204Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# And remove the remaining rows with missing values\n",
    "data = data.dropna()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.669753Z",
     "start_time": "2019-12-10T17:20:09.431476Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now we save the cleaned dataset it into a CSV file\n",
    "data.to_csv(os.path.join(path_data, 'gro_cleaned.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The net income columns is very weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.677310Z",
     "start_time": "2019-12-10T17:20:09.671806Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.689189Z",
     "start_time": "2019-12-10T17:20:09.679641Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "income = data['Net_Annual_Income']\n",
    "income.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The net income has a very weird distribution: a non-negligible number of very large values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:09.699068Z",
     "start_time": "2019-12-10T17:20:09.691306Z"
    }
   },
   "outputs": [],
   "source": [
    "(income <= 100).sum(), (income > 100).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most values are smaller than 100, while some are much larger..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:10.035759Z",
     "start_time": "2019-12-10T17:20:09.701420Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:10.248115Z",
     "start_time": "2019-12-10T17:20:10.037966Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "_ = plt.hist(income, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:10.574770Z",
     "start_time": "2019-12-10T17:20:10.250591Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(income[income <= 1000])\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(income[income > 1000])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:10.824761Z",
     "start_time": "2019-12-10T17:20:10.577507Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.distplot(income[income < 300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems that there's a bug in the net income features. \n",
    "Some numbers are in kilo euros, while others are in euros.\n",
    "Let's clean this by removing the rows with an income larger than 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:11.071447Z",
     "start_time": "2019-12-10T17:20:10.827602Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.distplot(income[income < 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:11.084427Z",
     "start_time": "2019-12-10T17:20:11.075030Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data[income <= 200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some data visualization with `pandas` + `seaborn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:11.563743Z",
     "start_time": "2019-12-10T17:20:11.086611Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.stripplot(x='Educational_Level', y='Net_Annual_Income', hue='Y', \n",
    "              jitter=True, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:11.893913Z",
     "start_time": "2019-12-10T17:20:11.566894Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Educational_Level', y='Net_Annual_Income', \n",
    "            hue='Y', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:12.207751Z",
     "start_time": "2019-12-10T17:20:11.896585Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(x='Marital_Status', y='Net_Annual_Income', \n",
    "               hue='Y', split=True, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:12.397283Z",
     "start_time": "2019-12-10T17:20:12.210301Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x='Marital_Status', hue='Y', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:14.007804Z",
     "start_time": "2019-12-10T17:20:12.399642Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(16, 16))\n",
    "columns = ['Customer_Type', 'P_Client', 'Educational_Level', \n",
    "           'Number_Of_Dependant', 'Marital_Status', 'Prod_Sub_Category',\n",
    "           'Source', 'Type_Of_Residence', 'Nb_Of_Products', \n",
    "           'Prod_Category', 'Y']\n",
    "\n",
    "for i, colname in enumerate(columns):\n",
    "    sns.countplot(colname, data=data, ax=fig.axes[i])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final preparation of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:14.016350Z",
     "start_time": "2019-12-10T17:20:14.010487Z"
    }
   },
   "outputs": [],
   "source": [
    "# First we make lists of continuous, categorial and date features\n",
    "\n",
    "cnt_featnames = [\n",
    "    'Years_At_Residence',\n",
    "    'Net_Annual_Income',\n",
    "    'Years_At_Business',\n",
    "    'Number_Of_Dependant'\n",
    "]\n",
    "\n",
    "cat_featnames = [\n",
    "    'Customer_Type',\n",
    "    'P_Client',\n",
    "    'Educational_Level',\n",
    "    'Marital_Status',\n",
    "    'Prod_Sub_Category',\n",
    "    'Source',\n",
    "    'Type_Of_Residence',\n",
    "    'Prod_Category',\n",
    "    'Nb_Of_Products'\n",
    "]\n",
    "\n",
    "date_featnames = [\n",
    "    'BirthDate',\n",
    "    'Customer_Open_Date',\n",
    "    'Prod_Decision_Date'\n",
    "#    'Prod_Closed_Date'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:14.033428Z",
     "start_time": "2019-12-10T17:20:14.018859Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data[cnt_featnames].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:14.062301Z",
     "start_time": "2019-12-10T17:20:14.036003Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bin_features = pd.get_dummies(data[cat_featnames],\n",
    "                              prefix_sep='#', drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:14.086752Z",
     "start_time": "2019-12-10T17:20:14.064760Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bin_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:20:14.100976Z",
     "start_time": "2019-12-10T17:20:14.089469Z"
    }
   },
   "outputs": [],
   "source": [
    "cnt_features = data[cnt_featnames]\n",
    "cnt_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:35:27.463009Z",
     "start_time": "2019-12-10T17:35:27.444735Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pandas import Timestamp\n",
    "\n",
    "def age(x):\n",
    "    today = Timestamp.today()\n",
    "    return (today - x).dt.days\n",
    "\n",
    "date_features = data[date_featnames].apply(age, axis=0)\n",
    "date_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:35:29.549153Z",
     "start_time": "2019-12-10T17:35:29.543190Z"
    }
   },
   "outputs": [],
   "source": [
    "all_features = pd.concat([bin_features, cnt_features, date_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:35:29.754096Z",
     "start_time": "2019-12-10T17:35:29.748490Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:35:30.879135Z",
     "start_time": "2019-12-10T17:35:30.850596Z"
    }
   },
   "outputs": [],
   "source": [
    "all_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VERY IMPORTANT**: we removed lines of data that contained missing values. The index of the dataframe is\n",
    "    therefore not contiguous anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:35:31.937186Z",
     "start_time": "2019-12-10T17:35:31.932001Z"
    }
   },
   "outputs": [],
   "source": [
    "all_features.index.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could be a problem for later. So let's reset the index to get a contiguous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:35:32.314890Z",
     "start_time": "2019-12-10T17:35:32.310553Z"
    }
   },
   "outputs": [],
   "source": [
    "all_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:35:32.518504Z",
     "start_time": "2019-12-10T17:35:32.515212Z"
    }
   },
   "outputs": [],
   "source": [
    "all_features.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:40:41.605000Z",
     "start_time": "2019-12-10T17:40:41.580822Z"
    }
   },
   "outputs": [],
   "source": [
    "all_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's save the data using `pickle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:35:34.463909Z",
     "start_time": "2019-12-10T17:35:34.451519Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "X = all_features\n",
    "y = data['Y']\n",
    "\n",
    "data_pkl = {}\n",
    "data_pkl['features'] = X\n",
    "data_pkl['labels'] = y\n",
    "\n",
    "data_pkl['cnt_featnames'] = cnt_featnames\n",
    "data_pkl['cat_featnames'] = cat_featnames\n",
    "data_pkl['date_featnames'] = date_featnames\n",
    "\n",
    "with open(os.path.join(path_data, 'gro_training.pkl'), 'wb') as f:\n",
    "    pkl.dump(data_pkl, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessed data is saved in a pickle file called `gro_training.pkl`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another approach using `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:42:47.967776Z",
     "start_time": "2019-12-10T17:42:47.903519Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# First we create a scikit-learn encoder that computes \n",
    "#  the age in days of columns containing dates\n",
    "class AgeEncoder(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        self.today = Timestamp.today()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.apply(lambda x: (x - self.today).dt.days, axis=0)\n",
    "\n",
    "# Centers and reduces (variance=1) columns\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "# One-hot encode, similar to pd.get_dummies\n",
    "one_hot_encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "\n",
    "# A pipeline that first computes age, and standardizes it\n",
    "scaled_age_encoder = Pipeline([\n",
    "    ('age', AgeEncoder()),\n",
    "    ('scaling', StandardScaler())\n",
    "])\n",
    "\n",
    "# Let's combine all these transformations\n",
    "transformer = ColumnTransformer([\n",
    "    ('standard_scaling', standard_scaler, cnt_featnames),\n",
    "    ('one_hot_encoding', one_hot_encoder, cat_featnames),\n",
    "    ('dates_age_scaled', scaled_age_encoder, date_featnames)\n",
    "])\n",
    "\n",
    "transformer.fit_transform(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
